# Runner organization
organization: "kubiya-test-sniff"

# Runner UUID
kubiyaAgentUUID: "679adc53-7068-4454-aa9f-16df30b14a50"

# NATS credentials and destination subject
nats:
  jwt: "xxx"
  secondJwt: "yyy"
  subject: "kubiya-test-sniff.sergey-metrics-test.incoming"
  serverUrl: "tls://connect.ngs.global"

# TLS certificates for private registry (used by `tool-manager`)
registryTls:
  crt: "xxx"
  key: "yyy"

# By default, runner name is generated from release name, override if needed
# will be propogated as a value to otel-collector configMap -> runner label
runnerNameOverride: "test-runner-chart"

# Optional secret for pulling images from private registries
imagePullSecrets: []

# kube-state-metrics subchart configuration
kubeStateMetrics:
  enabled: true

  # Default values for kube-state-metrics.
  image:
    tag: "2.14.0"
  updateStrategy: Recreate

  revisionHistoryLimit: 2

  # TODO keep if rbac role is not applied or metrics are not collected
  # namespaces: "kubiya"
  releaseNamespace: true
  rbac:
    useClusterRole: false

  # Explicit resources are mandatory. See README.md or more info.
  # Particular values below should be reviewed and set based on real usage statistics for particular runner (deployment) or average.

  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 10m
      memory: 32Mi

  selfMonitor:
    enabled: true
    telemetryHost: 0.0.0.0
    telemetryPort: 8081

# Kubiya operator runner component config
kubiyaOperator:
  create: true
  fullAccess: false # Controls whether operator has full access to all API groups and resources in runner's namespace
  replicas: 1
  serviceAccount:
    create: true
    name: "" # Leave empty when create is true to use the default generated name
    automountServiceAccountToken: true
  image:
    repository: ghcr.io/kubiyabot/kubiya-operator
    pullPolicy: IfNotPresent
    tag: runner_v2
  volumeMounts:
  - name: runner-secret-volume
    mountPath: "/etc/nats"
    readOnly: true
  volumes:
  - name: runner-secret-volume
    secret:
      secretName: kubiya-runner-nats-creds-runner
      items:
      - key: nats.creds
        path: nats.creds
  ports:
    containerPort: 80
  env:
    NATS_SERVER: "{{ .Values.nats.serverUrl }}"
    NATS_SUBJECT: "{{ .Values.nats.subject }}"
    NATS_CREDENTIAL_FILE: "/etc/nats/nats.creds"
    NAMESPACE: "{{ .Release.Namespace }}"

# Kubiya agent manager component config
agentManager:
  replicas: 2
  image:
    repository: ghcr.io/kubiyabot/agent-manager
    pullPolicy: IfNotPresent
    tag: 0.0.17
  service:
    port: 80
    targetPort: 8080
    serviceType: "ClusterIP"
  serviceAccount:
    create: true
    name: "" # Leave empty when create is true to use the default generated name
    automountServiceAccountToken: true
  # Explicit resources are mandatory. See README.md or more info.
  # Particular values below should be reviewed and set based on real usage statistics for particular runner (deployment) or average.
  resources:
    limits:
      cpu: "1"
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 256Mi
  volumes:
  - name: nats-creds-volume
    secret:
      defaultMode: 420
      items:
      - key: nats.creds
        path: nats.creds
      secretName: kubiya-runner-nats-creds-customer
  volumeMounts:
  - mountPath: /nats/
    name: nats-creds-volume
    readOnly: true
  env:
    TOOL_MANAGER_URL: "tool-manager:{{ .Values.toolManager.service.targetPort }}"
    KUBIYA_NATS_CREDS: "/nats/nats.creds"
    ENVIRONMENT: "production"

# Kubiya tool manager component config
toolManager:
  # tool-manager container config
  image:
    repository: ghcr.io/kubiyabot/tool-manager
    pullPolicy: IfNotPresent
    # TODO Point to merged branch release (with support for DAGGER_ENGINE_SERVICE_URL env var
    tag: 8763bd230593067d0f4e27775128cfbf70c1b5d2
  service:
    port: 80
    targetPort: 3001
    type: "ClusterIP"
  replicas: 3
  serviceAccount:
    create: true
    name: "kubiya-runner-tool-manager" # Falling to hardcod here, as cannot call helper generative function for setting KUBIYA_SERVICE_ACCOUNT env vars below.
    automountServiceAccountToken: true
  volumes:
  - name: nats-creds-volume
    secret:
      defaultMode: 420
      items:
      - key: nats.creds
        path: nats.creds
      secretName: kubiya-runner-nats-creds-customer
  - name: shared-volume
    emptyDir: {}
  - name: registry-certs
    secret:
      secretName: kubiya-runner-registry-tls-secret
      defaultMode: 420
  containerPort: 3001
  env:
    GIT_SHA: git-sha-placeholder
    KUBIYA_SERVICE_ACCOUNT: "{{ .Values.toolManager.serviceAccount.name }}"
    KUBIYA_TOOL_TIMEOUT: 168h
    KUBIYA_NATS_CREDS: "/opt/nats.creds"
    KUBIYA_USER_ORG: "{{ .Values.organization }}"
    KUBIYA_AGENT_UUID: "{{ .Values.kubiyaAgentUUID }}"
    KUBIYA_DAGGER_ENGINE_SERVICE_URL: "tcp://{{ .Values.daggerAdditionalResources.service.name }}:{{ .Values.daggerAdditionalResources.service.port }}"
    KUBIYA_SDK_SERVER_URL: "http://127.0.0.1:8000"
    KUBIYA_TOOLS_SHARED_VOLUME: "/tmp/kubiya_shared_tools"
    KUBIYA_IMAGE_REGISTRY_ADDRESS: "cache-registry-svc.kubiya"
  # Explicit resources are mandatory. See README.md or more info.
  # Particular values below should be reviewed and set based on real usage statistics for particular runner (deployment) or average.
  resources:
    limits:
      cpu: "1"
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 256Mi
  volumeMounts:
  - mountPath: /opt/
    name: nats-creds-volume
  - mountPath: /tmp/kubiya_shared_tools
    name: shared-volume
  - mountPath: /etc/docker/certs.d/cache-registry-svc.kubiya
    name: registry-certs
  command:
  - "/bin/bash"
  - "-c"
  - "mkdir -p /usr/local/share/ca-certificates && cp /etc/docker/certs.d/cache-registry-svc.kubiya/tls.crt /usr/local/share/ca-certificates/registry.crt && update-ca-certificates && /bin/bash /start_tools_server.sh"
  # sdk-server container config
  sdkServer:
    image:
      repository: ghcr.io/kubiyabot/sdk-py
      pullPolicy: IfNotPresent
      tag: v0.47.1
    # Explicit resources are mandatory. See README.md or more info.
    # Particular values below should be reviewed and set based on real usage statistics for particular runner (deployment) or average.
    resources:
      limits:
        cpu: "500m"
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 128Mi
    command:
    - "python"
    - "-m"
    - "kubiya_sdk"
    - "server"
    - "--host"
    - "0.0.0.0"
    - "--port"
    - "8000"
    containerPort: 8000
    defaultMode: 420
    env:
      KUBIYA_NATS_CREDS: "/opt/nats.creds"
      KUBIYA_USER_ORG: "{{ .Values.organization }}"
      KUBIYA_AGENT_UUID: "{{ .Values.kubiyaAgentUUID }}"
      GIT_SHA: git-sha-placeholder
      KUBIYA_SERVICE_ACCOUNT: "{{ .Values.toolManager.serviceAccount.name }}"
      KUBIYA_TOOL_TIMEOUT: 168h
      KUBIYA_TOOLS_SHARED_VOLUME: "/tmp/kubiya_shared_tools"
    volumeMounts:
    - mountPath: /opt/
      name: nats-creds-volume
    - mountPath: /tmp/kubiya_shared_tools
      name: shared-volume

# Kubiya image updater component config
imageUpdater:
  cronJob:
    # TODO: remove after testing
    create: true
  image:
    repository: bitnami/kubectl
    pullPolicy: IfNotPresent
    tag: 1.30.6
  serviceAccount:
    create: true
    name: "" # Leave empty when create is true to use the default generated name
    automountServiceAccountToken: true

# Use custom otel-collector Role with limited permissions instead of only ClusterRole from subchart
otelcollectorCustomRole: true

# Open-teemetry collector subchart config
opentelemetryCollector:
  enabled: true

  ports:
    metrics:
      enabled: true
      containerPort: 13133
      servicePort: 13133
      hostPort: 13133
      protocol: TCP

  image:
    # Using image build with custom implementation of nats-exporter to be able to push
    # telemetry and metrics from NATS server.
    # If you want to use the core image `otel/opentelemetry-collector`, you also need to change `command.name` value to `otelcol`.
    repository: ghcr.io/kubiyabot/otel-connector
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: release-0.1.1

  mode: deployment

  replicaCount: 1

  command:
    name: "otelcol-kubiya"
    extraArgs: [ "--config=/etc/otel/config.yaml" ]

  extraVolumes:
  - name: otel-config
    configMap:
      name: kubiya-runner-otel-collector-config
      items:
      - key: config.yaml
        path: config.yaml
  - name: nats-creds-volume
    secret:
      secretName: kubiya-runner-nats-creds-runner
      items:
      - key: nats.creds
        path: nats.creds

  extraVolumeMounts:
  - name: otel-config
    mountPath: /etc/otel
  - name: nats-creds-volume
    mountPath: "/etc/nats"
    readOnly: true
  configMap:
    create: false
  # Explicit resources are mandatory. See README.md or more info.
  # Particular values below should be reviewed and set based on real usage statistics for particular runner (deployment) or average.
  resources:
    limits:
      cpu: "2"
      memory: 4Gi
    requests:
      cpu: 250m
      memory: 256Mi

  # Adding Role and RoleBinding with least permissions for limit access to clients cluster resources. 
  serviceAccount:
    create: true
    name: "kubiya-runner-otel-collector"
  # Enable if need to use clusterRole
  # clusterRole:
  # create: false
  # name: ""
  # rules: []
  # # - apiGroups:
  # #   - ''
  # #   resources:
  # #   - 'pods'
  # #   - 'nodes'
  # #   verbs:
  # #   - 'get'
  # #   - 'list'
  # #   - 'watch'
  # clusterRoleBinding:
  # name: ""

dagger:
  enabled: true

  engine:
    newServiceAccount:
      name: "kubiya-runner-dagger-helm"
      create: true

    image:
      # Previous used version: 0.11.6 (?)
      # ref: registry.dagger.io/engine:v0.11.6

      # Latest vendor provided version: 0.13.6
      ref: registry.dagger.io/engine:v0.13.6

      # From update with headless svc template: v0.1.1
      # ref: ghcr.io/kubiyabot/kubiya-registry:v0.1.1

      args:
      - --oci-max-parallelism
      - num-cpu

    config: |
      # TODO: remove debug when tested - performance penalty?
      debug = true
      insecure-entitlements = ["security.insecure"]
      [registry."ghcr.io"]
        http = true
      [registry."ttl.sh"]
        http = true
      [registry."docker.io"]
        http = true
      [registry."cache-registry-svc.kubiya"]
      [log]
        format = "json"
      [grpc]
        address = ["unix:///run/buildkit/buildkitd.sock", "tcp://0.0.0.0:8080"]

    # Explicit resources are mandatory. See README.md or more info.
    # Particular values below should be reviewed and set based on real usage statistics for particular runner (deployment) or average.
    resources:
      limits:
        cpu: "4"
        memory: 4Gi
      requests:
        cpu: 100m
        memory: 128Mi

  magicache:
    # Secret must be created manually in the runner namespace per each client side runner.
    enabled: false
    # url: https://api.dagger.cloud/magicache
    # token: YOUR_DAGGER_CLOUD_TOKEN
    # secretName: EXISTING_SECRET_NAME

    # Extras for weak dagger subchart 
daggerAdditionalResources:
  # Role & RoleBinding for dagger engine.
  # Not created by default via dagger subchart, they still may be required for some use cases.
  # Transfered as-is from original runner template. If enabled will be attached to dagger.serviceAccount.name specified above.
  role:
    create: false
  roleBinding:
    create: false
  # headless service for dagger engine discovery and load balancing used by tool-manager
  # Dagger additional headless service port for dagger pods discovery and load balancing
  # At moment of writing this, dagger subchart does not provide headless service, while it's required for tool-manager (for now)to do load balancing
  # TODO: make sure port matches dagger container
  service:
    create: true
    # Name set to match currently hardcoded service name in tool-manager implementation.
    # In future tool-manager expected to be refactored to get service name from container env_vars.
    # TODO: If not set, will be auto generated as "<chart-name>-dagger-engine"
    name: dagger-engine-service
    port: 8080
