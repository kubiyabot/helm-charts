# Runner organization
organization: "kubiya-test-sniff"

# Runner UUID
kubiyaAgentUUID: "679adc53-7068-4454-aa9f-16df30b14a50"

# Dagger additional headless service port for dagger pods discovery and load balancing
# At moment of writing this, dagger subchart does not provide headless service, while it's required for tool-manager (for now)to do load balancing
#TODO: make sure port matches dagger container
daggerDiscoveryServicePort: 8080

# NATS credentials and destination subject
nats:
  jwt: "xxx"
  secondJwt: "yyy"
  subject: "kubiya-test-sniff.sergey-metrics-test.incoming"

# TODO: WTF???
registryTls:
  crt: "xxx"
  key: "yyy"

# By default, runner name is generated from release name, override if needed
# will be propogated as a value to otel-collector configMap -> runner label
runnerNameOverride: ""

# Optional secret for pulling images from private registries
imagePullSecrets: []

# kube-state-metrics subchart configuration
kubeStateMetrics:
  enabled: true

  # Default values for kube-state-metrics.
  image:
    tag: "2.14.0"
  updateStrategy: Recreate

  revisionHistoryLimit: 2
  # TODO keep if rbac role is not applied or metrics are not collected
  # namespaces: "kubiya"
  releaseNamespace: true
  rbac:
    useClusterRole: false
    # Resources defined here must be reviewed and set based on real usage statistics for particular runner deployment.
    # Apart from resource management goals, these are required for compliance checkl as well.

  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 10m
      memory: 32Mi

  selfMonitor:
    enabled: true
    telemetryHost: 0.0.0.0
    telemetryPort: 8081

# Kubiya operator runner component config
kubiyaOperator:
  create: true
  replicas: 1
  serviceAccount:
    create: true
    name: "" # Leave empty when create is true to use the default generated name
    automountServiceAccountToken: true
  image:
    repository: ghcr.io/kubiyabot/kubiya-operator
    pullPolicy: IfNotPresent
    tag: runner_v2
  volumeMounts:
  - name: runner-secret-volume
    mountPath: "/etc/nats"
    readOnly: true
  volumes:
  - name: runner-secret-volume
    secret:
      secretName: runner-secret # pragma: allowlist secret
      items:
      - key: nats.creds
        path: nats.creds
  ports:
    containerPort: 80

# Kubiya agent manager component config
agentManager:
  replicas: 2
  image:
    repository: ghcr.io/kubiyabot/agent-manager
    pullPolicy: IfNotPresent
    tag: 0.0.17
  service:
    port: 80
    targetPort: 8080
    serviceType: "ClusterIP"
  serviceAccount:
    create: true
    name: "" # Leave empty when create is true to use the default generated name
    automountServiceAccountToken: true
  # Resources defined here must be reviewed and set based on real usage statistics for particular runner deployment.
  # Apart from resource management goals, these are required for compliance checkl as well.
  resources:
    limits:
      cpu: "1"
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 256Mi
  volumes:
  - name: nats-creds-volume
    secret:
      defaultMode: 420
      items:
      - key: nats.creds
        path: nats.creds
      secretName: nats-creds-customer
  volumeMounts:
  - mountPath: /nats/
    name: nats-creds-volume
    readOnly: true

# Kubiya tool manager component config
toolManager:
  image:
    repository: ghcr.io/kubiyabot/tool-manager
    pullPolicy: Always
    tag: 462d60470b8f8063bac9c11887dc3620a71b8c56
  service:
    port: 80
    targetPort: 3001
    type: "ClusterIP"
  replicas: 3
  serviceAccount:
    create: true
    name: "kubiya-runner-tool-manager" # TODO: WTF? -- /Leave empty when create is true to use the default generated name
    automountServiceAccountToken: true
  volumes:
  - name: nats-creds-volume
    secret:
      defaultMode: 420
      items:
      - key: nats.creds
        path: nats.creds
      secretName: nats-creds-customer
  - name: shared-volume
    emptyDir: {}
  - name: registry-certs
    secret:
      secretName: registry-tls-secret
      defaultMode: 420
  # Resources defined here must be reviewed and set based on real usage statistics for particular runner deployment.
  # Apart from resource management goals, these are required for compliance checkl as well.
  resources:
    limits:
      cpu: "1"
      memory: 1Gi
    requests:
      cpu: 200m
      memory: 256Mi

  volumeMounts:
  - mountPath: /opt/
    name: nats-creds-volume
  - mountPath: /tmp/kubiya_shared_tools
    name: shared-volume
  - mountPath: /etc/docker/certs.d/cache-registry-svc.kubiya
    name: registry-certs
  command:
  - "/bin/bash"
  - "-c"
  - "mkdir -p /usr/local/share/ca-certificates && cp /etc/docker/certs.d/cache-registry-svc.kubiya/tls.crt /usr/local/share/ca-certificates/registry.crt && update-ca-certificates && /bin/bash /start_tools_server.sh"
  env:
    KUBIYA_NATS_CREDS: "/opt/nats.creds"
    KUBIYA_USER_ORG: "{{ .Values.organization }}"
    KUBIYA_AGENT_kubiyaAgentUUID: "{{ .Values.kubiyaAgentUUID }}"
    KUBIYA_SERVICE_ACCOUNT: kubiya-runner-tool-manager
    GIT_SHA: git-sha-placeholder
    KUBIYA_TOOL_TIMEOUT: 168h
    KUBIYA_SDK_SERVER_URL: "http://127.0.0.1:8000"
    KUBIYA_TOOLS_SHARED_VOLUME: "/tmp/kubiya_shared_tools"
    KUBIYA_IMAGE_REGISTRY_ADDRESS: "cache-registry-svc.kubiya"

  sdkServer:
    image:
      repository: ghcr.io/kubiyabot/sdk-py
      pullPolicy: IfNotPresent
      tag: v0.47.1
    # Resources defined here must be reviewed and set based on real usage statistics for particular runner deployment.
    # Apart from resource management goals, these are required for compliance checkl as well.
    resources:
      limits:
        cpu: "500m"
        memory: 512Mi
      requests:
        cpu: 100m
        memory: 128Mi
    command:
    - "python"
    - "-m"
    - "kubiya_sdk"
    - "server"
    - "--host"
    - "0.0.0.0"
    - "--port"
    - "8000"
    containerPort: 8000
    defaultMode: 420
    env:
      KUBIYA_NATS_CREDS: "/opt/nats.creds"
      KUBIYA_USER_ORG: "{{ .Values.organization }}"
      KUBIYA_AGENT_kubiyaAgentUUID: "{{ .Values.kubiyaAgentUUID }}"
      GIT_SHA: git-sha-placeholder
      KUBIYA_SERVICE_ACCOUNT: kubiya-runner-tool-manager
      KUBIYA_TOOL_TIMEOUT: 168h
      KUBIYA_TOOLS_SHARED_VOLUME: "/tmp/kubiya_shared_tools"
    volumeMounts:
    - mountPath: /opt/
      name: nats-creds-volume
    - mountPath: /tmp/kubiya_shared_tools
      name: shared-volume

# Kubiya image updater component config
imageUpdater:
  cronJob:
    create: true
  image:
    repository: bitnami/kubectl
    pullPolicy: IfNotPresent
    tag: 1.30.6
  serviceAccount:
    create: true
    name: "" # Leave empty when create is true to use the default generated name
    automountServiceAccountToken: true

# Open-teemetry collector subchart config
opentelemetry-collector:
  enabled: false

  # Using image build with custom implementation of nats-exporter to be able to push
  # telemetry and metrics from NATS server.
  image:
    # If you want to use the core image `otel/opentelemetry-collector`, you also need to change `command.name` value to `otelcol`.
    repository: ghcr.io/kubiyabot/otel-connector
    pullPolicy: IfNotPresent
    # Overrides the image tag whose default is the chart appVersion.
    tag: release-0.1.1

  mode: deployment

  # TODO: remove once tested with default command for our otel-collector image
  # command:
  #   name: "otelcol-k8s"

  replicaCount: 1

  configMap:
    create: false
    existingName: "otel-collector-config"
    existingKey: "otel-collector-config.yaml"

  # Resources defined here must be reviewed and set based on real usage statistics for particular runner deployment.
  # Apart from resource management goals, these are required for compliance checkl as well.
  resources:
    limits:
      cpu: "2"
      memory: 4Gi
    requests:
      cpu: 250m
      memory: 256Mi

  # NATS credentials secret
  extraVolumes:
  - name: nats-creds-volume
    secret:
      secretName: nats-creds # pragma: allowlist secret
      items:
      - key: nats.creds
        path: nats.creds
  extraVolumeMounts:
  - name: nats-creds-volume
    mountPath: "/opt"
    readOnly: true
  # TODO: remove if permissions allow otel-collector collect defined data without clusterRole
  # or add Role and RoleBinding from original runner template, to avoid ClusterRole
  serviceAccount:
    create: true
  # clusterRole:
  # create: false
  # name: ""
  # rules: []
  # # - apiGroups:
  # #   - ''
  # #   resources:
  # #   - 'pods'
  # #   - 'nodes'
  # #   verbs:
  # #   - 'get'
  # #   - 'list'
  # #   - 'watch'
  # clusterRoleBinding:
  # name: ""

dagger:
  enabled: true

  engine:
    image:
      # Previous used version: 0.11.6 (?)
      # ref: registry.dagger.io/engine:v0.11.6

      # Latest vendor provided version: 0.13.6
      # ref: registry.dagger.io/engine:v0.13.6

      # From update with headless svc template: v0.1.1
      ref: ghcr.io/kubiyabot/kubiya-registry:v0.1.1

    config: |
      # TODO: remove debug when tested - performance penalty?
      debug = true
      insecure-entitlements = ["security.insecure"]
      [registry."ghcr.io"]
        http = true
      [registry."ttl.sh"]
        http = true
      [registry."docker.io"]
        http = true
      [registry."cache-registry-svc.kubiya"]
      [log]
        format = "json"
      [grpc]
        address = ["unix:///run/buildkit/buildkitd.sock", "tcp://0.0.0.0:8080"]

    # Resources defined here must be reviewed and set based on real usage statistics for particular runner deployment.
    # Apart from resource management goals, these are required for compliance checkl as well.
    resources:
      limits:
        cpu: "4"
        memory: 4Gi
      requests:
        cpu: "1"
        memory: 1Gi

  magicache:
    # Secret must be created manually in the runner namespace per each client side runner.
    enabled: false
    # url: https://api.dagger.cloud/magicache
    # token: YOUR_DAGGER_CLOUD_TOKEN
    # secretName: EXISTING_SECRET_NAME

  newServiceAccount:
    name: "kubiya-runner-dagger-helm"
    create: true
