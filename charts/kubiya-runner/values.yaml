# Runner organization
organization: "testOrg"
# Runner UUID
uuid: "f47ac10b-58cc-4372-a567-0e02b2c3d479"
# By default, runner name = release name, override if needed. Must match JWT encoded (see README.md for details)
runnerNameOverride: ""
nameOverride: ""
fullnameOverride: ""
# Optional secret for pulling images from private registries
imagePullSecrets: []
# NATS credentials and destination subject
nats:
  jwt: ""
  secondJwt: ""
  subject: ""
  serverUrl: "tls://connect.ngs.global"
# TLS certificates for private registry (used by `tool-manager`)
registryTls:
  crt: "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQ4VENDQXRtZ0F3SUJBZ0lVTVBIY1RMN1FGNmMrYlhtNElxTEUzeXVhK1lrd0RRWUpLb1pJaHZjTkFRRUwKQlFBd2NURUxNQWtHQTFVRUJoTUNWVk14Q3pBSkJnTlZCQWdNQWtOQk1SSXdFQVlEVlFRSERBbFRkVzV1ZVhaaApiR1V4RHpBTkJnTlZCQW9NQmt0MVltbDVZVEVNTUFvR0ExVUVDd3dEVWs1RU1TSXdJQVlEVlFRRERCbGpZV05vClpTMXlaV2RwYzNSeWVTMXpkbU11YTNWaWFYbGhNQjRYRFRJME1UQXdNakV5TkRZeU4xb1hEVEkzTURrd01qRXkKTkRZeU4xb3djVEVMTUFrR0ExVUVCaE1DVlZNeEN6QUpCZ05WQkFnTUFrTkJNUkl3RUFZRFZRUUhEQWxUZFc1dQplWFpoYkdVeER6QU5CZ05WQkFvTUJrdDFZbWw1WVRFTU1Bb0dBMVVFQ3d3RFVrNUVNU0l3SUFZRFZRUUREQmxqCllXTm9aUzF5WldkcGMzUnllUzF6ZG1NdWEzVmlhWGxoTUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEEKTUlJQkNnS0NBUUVBM3c5b2dVUGptaVNaMDR1OU4vR3hKRmhCT01QYzQ3MXJLR1ozZVc2bW43NW1FTjNzV2R1cwpzVStkK1lNSE9HYjBUT3FlcHhMUUtvMWNRYjVRUmlRTGxOVTJWRGgrTjVhZVVmdVJUU1MwczNRWTAxd1k1Z3ozCnIvaFc3ZllEOVVpTUpTeFhYK01DU2hsQ0NTUm52N2hncmloQWlGdXcyZUs3bjQyWjByYms2MUJmaGNBRWxZbmEKdExoRjBwcWxvNGlsSHZaUGEwdzRFMzJBdkJJTzZ5YjlKb1V0N1JRMVNta3VLRDJsY2dqMVlKSjQ2SmxKUGtFWgowL21Zc3ZwZUZBOVBmQnZGdkZ6K0VyMitMd09JKzcxNFRtTnNITXVERjRyNzVNeFF5ekpaYTNxT3Rud3paTEVYCjlHYThHZ1AxVG9jVGtlRnVMUmhjVWQ4M3M5eWJIS0o1bHdJREFRQUJvNEdBTUg0d0RnWURWUjBQQVFIL0JBUUQKQWdXZ01CTUdBMVVkSlFRTU1Bb0dDQ3NHQVFVRkJ3TUJNRGdHQTFVZEVRUXhNQytDR1dOaFkyaGxMWEpsWjJsegpkSEo1TFhOMll5NXJkV0pwZVdHQ0VtTmhZMmhsTFhKbFoybHpkSEo1TFhOMll6QWRCZ05WSFE0RUZnUVVxQlpGCkZKZEZLQ1M0SGtSYWxtNmM1WFFORlJVd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFHYXlraDhhV3QwaTdpNnMKOGZZRHlZVE8wMjZOYjQ5dWNUZDRpKzgySGFCdm01ZFJxbVQ5U0V4eFJYL3RTanMraGpHRnpYTWVtY0lva3FKLwpFNWtSYlcrbFNveWUwQkdkTVBPcVJNZW1tcEI4WG9DZmo0ZWg0WkxtbDRrbGVSZEEzRU96OHJSRExxb0MxdmJiClluU2RwTjFHbUFJOUtpdjJ0N3hjVVl0Nm1SM2FqYVBLZHVWOXNkakRrZDBDQm1EckhNRXJ4a3AzdDVXOHc5aW8KN2IxRjE3MFBSRldJNVY3c3JtQlFwK0t6UTJsZEdyODdxZFI1eE1wZlU4Z0IvKzgyRXlZcjA1WTJsNmhrY1BGbgpzMnBPTUV4UFVVN05xQlMxeURoR3lXZVVJTE9ydmFuWjh3L1UyVDFScjd1bTBGK0Y0a3hTMHZNMDhyYmRIMzNPCjVyUDV1OEk9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K" # pragma: allowlist secret
  key: "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" # pragma: allowlist secret
# ---------------------------------------- Kubiya Runner Components Configuration ---------------------------------------- 

# Kubiya operator runner component config
kubiyaOperator:
  create: true
  fullAccess: false # Controls whether operator has full access to all API groups and resources in runner's namespace
  replicas: 1
  serviceAccount:
    create: true
    name: "" # Leave empty when create is true to use the default generated name
    automountServiceAccountToken: true
  image:
    repository: ghcr.io/kubiyabot/kubiya-operator
    pullPolicy: IfNotPresent
    tag: runner_v2
  # TODO/PROD_READY: Enable when kubiya-operator will have it
  livenessProbe:
    enabled: false
    # httpGet:
    #   path: /health
    # initialDelaySeconds: 30
    # periodSeconds: 30
    # timeoutSeconds: 5
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 64Mi
  volumeMounts:
    - name: runner-secret-volume
      mountPath: "/etc/nats"
      readOnly: true
  volumes:
    - name: runner-secret-volume
      secret:
        secretName: kubiya-runner-nats-creds-runner
        items:
          - key: nats.creds
            path: nats.creds
  ports:
    containerPort: 80
  env:
    NATS_SERVER: "{{ .Values.nats.serverUrl }}"
    NATS_SUBJECT: "{{ .Values.nats.subject }}"
    NATS_CREDENTIAL_FILE: "/etc/nats/nats.creds"
    NAMESPACE: "{{ .Release.Namespace }}"
# Kubiya agent manager component config
agentManager:
  replicas: 2
  image:
    repository: ghcr.io/kubiyabot/agent-manager
    pullPolicy: IfNotPresent
    tag: v0.1.6
  service:
    port: 80
    targetPort: 8080
    serviceType: "ClusterIP"
  serviceAccount:
    create: true
    name: "" # Leave empty when create is true to use the default generated name
    automountServiceAccountToken: true
  livenessProbe:
    enabled: true
    httpGet:
      path: /healthz
    initialDelaySeconds: 30
    periodSeconds: 30
    timeoutSeconds: 10 # Need to be reviewed and set by service owner
  # Add annotations for prometheus/alloy to scrape metrics
  extraPodsAnnotations:
    prometheus.io/scrape: "true"
  # Explicit resources are mandatory. See README.md or more info.
  # Particular values below should be reviewed and set based on real usage statistics for particular runner (deployment) or average.
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 64Mi
  volumes:
    - name: nats-creds-volume
      secret:
        defaultMode: 420
        items:
          - key: nats.creds
            path: nats.creds
        secretName: kubiya-runner-nats-creds-customer
  volumeMounts:
    - mountPath: /nats/
      name: nats-creds-volume
      readOnly: true
  env:
    TOOL_MANAGER_URL: "http://tool-manager.{{ .Release.Namespace }}:{{ .Values.toolManager.service.port }}"
    KUBIYA_NATS_CREDS: "/nats/nats.creds"
    ENVIRONMENT: "production"
# Kubiya tool manager component config
toolManager:
  # tool-manager container config
  image:
    repository: ghcr.io/kubiyabot/tool-manager
    pullPolicy: IfNotPresent
    tag: v0.3.12
  otelCollectorMetricsScrapeAppName: tool-manager
  service:
    port: 80
    targetPort: 3001
    type: "ClusterIP"
  replicas: 3
  serviceAccount:
    create: true
    name: "kubiya-runner-tool-manager" # Falling to hardcod here, as cannot call helper generative function for setting KUBIYA_SERVICE_ACCOUNT env vars below.
    automountServiceAccountToken: true
  adminClusterRole:
    create: false # Set to true to create the ClusterRole and ClusterRoleBinding
  # Add annotations for prometheus to scrape metrics
  extraPodsAnnotations:
    prometheus.io/scrape: "true"
  livenessProbe:
    enabled: true
    httpGet:
      path: /health # ask to rename to /healthz
    initialDelaySeconds: 30
    periodSeconds: 30
    timeoutSeconds: 5
  volumes:
    - name: nats-creds-volume
      secret:
        defaultMode: 420
        items:
          - key: nats.creds
            path: nats.creds
        secretName: kubiya-runner-nats-creds-customer
    - name: shared-volume
      emptyDir: {}
    - name: registry-certs
      secret:
        secretName: registry-tls-secret
        defaultMode: 420
  containerPort: 3001
  env:
    GIT_SHA: git-sha-placeholder
    KUBIYA_SERVICE_ACCOUNT: "{{ .Values.toolManager.serviceAccount.name }}"
    KUBIYA_TOOL_TIMEOUT: 5m
    KUBIYA_LONG_RUNNING_TOOL_TIMEOUT: 168h
    KUBIYA_NATS_CREDS: "/opt/nats.creds"
    KUBIYA_USER_ORG: "{{ .Values.organization }}"
    # TODO: FIX AFTER MERGE
    KUBIYA_AGENT_UUID: "{{ .Values.uuid }}"
    KUBIYA_DAGGER_ENGINE_SERVICE_URL: "tcp://{{ .Values.dagger.additionalResources.service.name }}:{{ .Values.dagger.additionalResources.service.port }}"
    KUBIYA_SDK_SERVER_URL: "http://127.0.0.1:8000"
    KUBIYA_TOOLS_SHARED_VOLUME: "/tmp/kubiya_shared_tools"
    KUBIYA_IMAGE_REGISTRY_ADDRESS: "cache-registry-svc.kubiya"
    KUBIYA_RUNNER_NAME: "{{ .Values.runner_name }}"
    KUBIYA_GROUP_NAME: "{{ .Values.organization }}.{{ .Values.runner_name }}"
  # Explicit resources are mandatory. See README.md or more info.
  # Particular values below should be reviewed and set based on real usage statistics for particular runner (deployment) or average.
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 64Mi
  volumeMounts:
    - mountPath: /opt/
      name: nats-creds-volume
    - mountPath: /tmp/kubiya_shared_tools
      name: shared-volume
    - mountPath: /etc/docker/certs.d/cache-registry-svc.kubiya
      name: registry-certs
  command:
    - "/bin/bash"
    - "-c"
    - "mkdir -p /usr/local/share/ca-certificates && cp /etc/docker/certs.d/cache-registry-svc.kubiya/tls.crt /usr/local/share/ca-certificates/registry.crt && update-ca-certificates && /bin/bash /start_tools_server.sh"
  # sdk-server container config
  sdkServer:
    image:
      repository: ghcr.io/kubiyabot/sdk-py
      pullPolicy: IfNotPresent
      tag: v1.12.0
    # Explicit resources are mandatory. See README.md or more info.
    # Particular values below should be reviewed and set based on real usage statistics for particular runner (deployment) or average.
    resources:
      limits:
        cpu: 1
        memory: 1Gi
      requests:
        cpu: 100m
        memory: 64Mi
    command:
      - "python"
      - "-m"
      - "kubiya_sdk"
      - "server"
      - "--host"
      - "0.0.0.0"
      - "--port"
      - "8000"
    containerPort: 8000
    defaultMode: 420
    livenessProbe:
      enabled: true
      httpGet:
        path: /health
      initialDelaySeconds: 30
      periodSeconds: 30
      timeoutSeconds: 5
    env:
      KUBIYA_NATS_CREDS: "/opt/nats.creds"
      KUBIYA_USER_ORG: "{{ .Values.organization }}"
      KUBIYA_AGENT_UUID: "{{ .Values.uuid }}"
      GIT_SHA: git-sha-placeholder
      KUBIYA_SERVICE_ACCOUNT: "{{ .Values.toolManager.serviceAccount.name }}"
      KUBIYA_TOOL_TIMEOUT: 5m
      KUBIYA_LONG_RUNNING_TOOL_TIMEOUT: 168h
      KUBIYA_TOOLS_SHARED_VOLUME: "/tmp/kubiya_shared_tools"
    volumeMounts:
      - mountPath: /opt/
        name: nats-creds-volume
      - mountPath: /tmp/kubiya_shared_tools
        name: shared-volume
# Kubiya image updater component config
imageUpdater:
  cronJob:
    create: true
    schedule: "0 0 * * *" # Default schedule
    suspend: false # Default suspend value
    successfulJobsHistoryLimit: 1
    failedJobsHistoryLimit: 1
  image:
    repository: ghcr.io/kubiyabot/kubernetes
    pullPolicy: IfNotPresent
    tag: 1.32.0
  serviceAccount:
    create: true
    name: "" # Leave empty when create is true to use the default generated name
    automountServiceAccountToken: true
# Dagger subchart config
dagger:
  enabled: true
  # Override fullname for dagger engine deployment to be able to use kustomize patch
  volumeMountsPostfix: "{{ .Values.uuid }}"
  engine:
    # Labels for selecting dagger pods for additional dagger headless service (for discovery and load balancing)
    # which is used by tool-manager.
    labels:
      app.kubernetes.io/kubiya-runner-component: dagger-engine
    newServiceAccount:
      name: "kubiya-runner-dagger-helm"
      create: true
    image:
      ref: ghcr.io/kubiyabot/kubiya-registry:v0.1.1
    args:
      - --oci-max-parallelism
      - num-cpu
    # Add "debug = true" when needed
    # Overriding readiness probe from forked dagger to match one from our custom dagger build
    readinessProbe:
      exec:
        command:
          - buildctl
          - debug
          - workers
    config: |
      insecure-entitlements = ["security.insecure"]
      [registry."ghcr.io"]
        http = true
      [registry."ttl.sh"]
        http = true
      [registry."docker.io"]
        http = true
      [registry."cache-registry-svc.kubiya"]
      [log]
        format = "json"
      [grpc]
        address = ["unix:///run/buildkit/buildkitd.sock", "tcp://0.0.0.0:8080"]
    # Explicit resources are mandatory. See README.md or more info.
    # Particular values below should be reviewed and set based on real usage statistics for particular runner (deployment) or average.
    resources:
      limits:
        cpu: 4
        memory: 4Gi
      requests:
        cpu: 100m
        memory: 128Mi
  magicache:
    enabled: false
  additionalResources:
    role:
      create: false
    roleBinding:
      create: false
    service:
      create: true
      name: dagger-engine-service
      port: 8080
# ---------------------------------------- Telemetry Components Configuration ---------------------------------------- 

# kube-state-metrics subchart configuration
kubestatemetrics:
  enabled: true
  ## kube-state-metrics endpoint scrape interval
  http:
    interval: "{{ .Values.alloy.scrapeInterval.default }}"
  ## kube-state-metrics metrics scrape interval
  metrics:
    interval: "{{ .Values.alloy.scrapeInterval.default }}"
  # Default values for kube-state-metrics.
  image:
    tag: "v2.14.0"
  updateStrategy: Recreate
  revisionHistoryLimit: 2
  metricLabelsAllowlist:
    - pods=[*]
  # TODO keep if rbac role is not applied or metrics are not collected
  # namespaces: "kubiya"
  releaseNamespace: true
  rbac:
    useClusterRole: false
  # Explicit resources are mandatory. See README.md or more info.
  # Particular values below should be reviewed and set based on real usage statistics for particular runner (deployment) or average.
  resources:
    limits:
      cpu: 200m
      memory: 256Mi
    requests:
      cpu: 10m
      memory: 32Mi
  selfMonitor:
    enabled: true
    telemetryHost: 0.0.0.0
    telemetryPort: 8081
alloy:
  enabled: true
  # Pipeline live debug via Alloy GUI (GUI needs port forward to alloy service, port 12345)
  livedebugging:
    enabled: false
  # Stage II - extra exporters. Disabled by default as not needed and not tested for first chart update release.
  extraExporters:
    cadvisor:
      enabled: false
  scrapeIntervals:
    default: 60s
    runnerExporters: 60s
    alloyExporter: 60s
    blackboxExporter: 60s
    kubeStateMetrics: 60s
    cadvisor: 60s # disabled by default
  # Alloy security context - running as non-root user
  alloy:
    # Experimental level required for Alloy live debugging to work. DO NOT use in production. 
    # stabilityLevel: "experimental"
    securityContext:
      runAsUser: 473
      runAsGroup: 473
    configMap:
      create: false
      key: config.alloy
      name: kubiya-runner-alloy-config
    extraEnv:
      - name: AZURE_REMOTE_WRITE_URL
        value: "xxx"
      - name: AZURE_CLIENT_ID
        value: "xxx"
      - name: AZURE_CLIENT_SECRET
        value: "xxx"
      - name: AZURE_TOKEN_URL
        value: "xxx"
  controller:
    podLabels:
      app.kubernetes.io/kubiya-runner-component: alloy
    type: deployment
  # Explicit resources are mandatory. See README.md or more info.
  # Particeular values below should be reviewed and set based on real usage statistics for particular runner (deployment) or average.
  resources:
    limits:
      cpu: 1
      memory: 1Gi
    requests:
      cpu: 100m
      memory: 128Mi
