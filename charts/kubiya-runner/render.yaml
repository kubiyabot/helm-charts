---
# Source: kubiya-runner/charts/kubeStateMetrics/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
automountServiceAccountToken: true
metadata:
  labels:    
    helm.sh/chart: kubeStateMetrics-5.27.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kubeStateMetrics
    app.kubernetes.io/name: kubeStateMetrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.14.0"
  name: release-name-kubeStateMetrics
  namespace: kubiya
---
# Source: kubiya-runner/charts/opentelemetry-collector/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: otel-collector-sa-token
  namespace: kubiya
  labels:
    helm.sh/chart: opentelemetry-collector-0.109.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.113.0"
    app.kubernetes.io/managed-by: Helm
---
# Source: kubiya-runner/templates/image-updater-serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: image-updater-sa
  namespace: kubiya
  labels:    
    helm.sh/chart: kubiya-runner-0.1.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: image-updater
---
# Source: kubiya-runner/templates/kubiya-operator-serviceAccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: kubiya-runner
  namespace: kubiya
  labels:
    helm.sh/chart: kubiya-runner-0.1.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: operator
automountServiceAccountToken: true
---
# Source: kubiya-runner/charts/dagger/templates/engine-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: release-name-dagger-engine-config
  namespace: kubiya
  labels:
    helm.sh/chart: dagger-0.13.6
    app.kubernetes.io/name: dagger
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: v0.13.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: dagger
data:
  engine.toml: |
    # debug = true
    insecure-entitlements = ["security.insecure"]
    [registry."ghcr.io"]
      http = true
    [registry."ttl.sh"]
      http = true
    [registry."docker.io"]
      http = true
    [registry."cache-registry-svc.kubiya"]
    [log]
      format = "json"
---
# Source: kubiya-runner/templates/otel-collector-configMap.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: otel-collector-config
  namespace: kubiya
  labels:
    helm.sh/chart: kubiya-runner-0.1.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: otel-collector
data:
  otel-collector-config.yaml: |
    # Otel-collector configuration file
    receivers:
      otlp:
        protocols:
          grpc:
            endpoint: 0.0.0.0:4317
          http:
            endpoint: 0.0.0.0:3000

      # Kubernetes API server (kube-apiserver) metrics via otel-collector receiver
      k8s_events:
        auth_type: serviceAccount
        namespaces:
          - kubiya;

      # Kubelet metrics via otel-collector receiver
      # TODO remove hardcoded namespace
      prometheus:
        config:
          scrape_configs:
          # Kube-state-metrics exporter metrics receiver
          - job_name: 'kube-state-metrics'
            scrape_interval: 30s
            static_configs:
              - targets: ['kube-state-metrics:8080']
            kubernetes_sd_configs:
              - role: endpoints
                namespaces:
                  names:
                    - kubiya
            relabel_configs:
              - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
                regex: kube-state-metrics
                action: keep
          
          # Dynamic service discovery for Kubiya apps, for all metrics exposed by services on default /metrics endpoint
          - job_name: "kubiya-apps"
            scrape_interval: 10s
            kubernetes_sd_configs:
              - role: service
                namespaces:
                  names: ["kubiya"]
            relabel_configs:
              # Keep only services we want to scrape
              # TODO add more once exporters are ready
              - source_labels: [__meta_kubernetes_service_name]
                action: keep
                regex: (tool-manager|agent-manager)
              # Add app label
              - source_labels: [__meta_kubernetes_service_name]
                target_label: app
              # Use the service port
              - source_labels: [__meta_kubernetes_service_port_number]
                target_label: __metrics_port__

    #TODO: remove hardcoded namespace
    processors:
      k8sattributes:
        auth_type: serviceAccount
        filter:
            namespace: kubiya
      
      batch:
      
      resource:
        attributes:
        # TODO: instance and cluster drop
          - key: "organization"
            value: 
            action: upsert
          - key: "runner"
            value: release-name
            action: upsert
            
      metricstransform:
        transforms:
          - include: '.*'
            match_type: regexp
            action: update
            operations:
              - action: update_label
                label: "organization"
                new_label: "organization"
                new_value: 
              - action: add_label
                label: "kubiya_type"
                new_label: "kubiya_type"
                new_value: "customers-runners"
              - action: add_label
                label: "runner"
                new_label: "runner"
                new_value:         release-name

    exporters:
      # Debug output for testing
      # debug:
      #   verbosity: normal
      #   sampling_initial: 5
      #   @sampling_thereafter: 200
      natsexporter:
        org: 
        endpoint: tls://connect.ngs.global
        creds_file: /etc/nats/nats.creds

    service:
      pipelines:
        logs:
          receivers: [ k8s_events ]
          processors: [ batch, resource ]
          exporters: [ natsexporter ]
        metrics:
          receivers: [ prometheus ]
          processors: [ metricstransform, k8sattributes, batch, resource ]
          exporters: [ natsexporter ]
        traces:
          receivers: [ otlp ]
          processors: [ batch ]
          exporters: [ natsexporter ]
      telemetry:
        logs:
---
# Source: kubiya-runner/charts/kubeStateMetrics/templates/role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  labels:    
    helm.sh/chart: kubeStateMetrics-5.27.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kubeStateMetrics
    app.kubernetes.io/name: kubeStateMetrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.14.0"
  name: release-name-kubeStateMetrics
  namespace: 
rules:

- apiGroups: ["certificates.k8s.io"]
  resources:
  - certificatesigningrequests
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - cronjobs
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - daemonsets
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - deployments
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - endpoints
  verbs: ["list", "watch"]

- apiGroups: ["autoscaling"]
  resources:
  - horizontalpodautoscalers
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "networking.k8s.io"]
  resources:
  - ingresses
  verbs: ["list", "watch"]

- apiGroups: ["batch"]
  resources:
  - jobs
  verbs: ["list", "watch"]

- apiGroups: ["coordination.k8s.io"]
  resources:
  - leases
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - limitranges
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - mutatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - namespaces
  verbs: ["list", "watch"]

- apiGroups: ["networking.k8s.io"]
  resources:
  - networkpolicies
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumeclaims
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - persistentvolumes
  verbs: ["list", "watch"]

- apiGroups: ["policy"]
  resources:
    - poddisruptionbudgets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "watch"]

- apiGroups: ["extensions", "apps"]
  resources:
  - replicasets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - replicationcontrollers
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - resourcequotas
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - secrets
  verbs: ["list", "watch"]

- apiGroups: [""]
  resources:
  - services
  verbs: ["list", "watch"]

- apiGroups: ["apps"]
  resources:
  - statefulsets
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - storageclasses
  verbs: ["list", "watch"]

- apiGroups: ["admissionregistration.k8s.io"]
  resources:
    - validatingwebhookconfigurations
  verbs: ["list", "watch"]

- apiGroups: ["storage.k8s.io"]
  resources:
    - volumeattachments
  verbs: ["list", "watch"]
---
# Source: kubiya-runner/templates/image-updater-role.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: kubiya-runner-deployment-updater
  namespace: kubiya
  labels:    
    helm.sh/chart: kubiya-runner-0.1.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: image-updater
rules:
- apiGroups: ["apps"]
  resources: ["deployments"]
  verbs: ["get", "patch"]
---
# Source: kubiya-runner/templates/kubiya-runner-role.yaml
kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubiya-runner-full-api-access
  namespace: kubiya
rules:
  - apiGroups: [ "*" ]
    resources: [ "*" ]
    verbs: [ "*" ]
---
# Source: kubiya-runner/charts/kubeStateMetrics/templates/rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  labels:    
    helm.sh/chart: kubeStateMetrics-5.27.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kubeStateMetrics
    app.kubernetes.io/name: kubeStateMetrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.14.0"
  name: release-name-kubeStateMetrics
  namespace: 
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: release-name-kubeStateMetrics
subjects:
- kind: ServiceAccount
  name: release-name-kubeStateMetrics
  namespace: kubiya
---
# Source: kubiya-runner/templates/image-updater-rolebinding.yaml
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: kubiya-runner-image-updater-rolebinding
  namespace: kubiya
  labels:    
    helm.sh/chart: kubiya-runner-0.1.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: image-updater
subjects:
- kind: ServiceAccount
  name: image-updater-sa
  namespace: kubiya
roleRef:
  kind: Role
  name: kubiya-runner-deployment-updater
  apiGroup: rbac.authorization.k8s.io
---
# Source: kubiya-runner/templates/kubiya-runner-roleBinding.yaml
kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: kubiya-runner-namespace-full-access
  namespace: kubiya
subjects:
  - kind: ServiceAccount
    name: kubiya-runner
    namespace: kubiya
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: kubiya-runner-full-api-access
---
# Source: kubiya-runner/charts/kubeStateMetrics/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-kubeStateMetrics
  namespace: kubiya
  labels:    
    helm.sh/chart: kubeStateMetrics-5.27.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kubeStateMetrics
    app.kubernetes.io/name: kubeStateMetrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.14.0"
  annotations:
    prometheus.io/scrape: 'true'
spec:
  type: "ClusterIP"
  ports:
  - name: "http"
    protocol: TCP
    port: 8080
    targetPort: 8080
  
  - name: "metrics"
    protocol: TCP
    port: 8081
    targetPort: 8081
  
  selector:    
    app.kubernetes.io/name: kubeStateMetrics
    app.kubernetes.io/instance: release-name
---
# Source: kubiya-runner/charts/opentelemetry-collector/templates/service.yaml
apiVersion: v1
kind: Service
metadata:
  name: release-name-opentelemetry-collector
  namespace: kubiya
  labels:
    helm.sh/chart: opentelemetry-collector-0.109.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.113.0"
    app.kubernetes.io/managed-by: Helm
    
    component: standalone-collector
spec:
  type: ClusterIP
  ports:
    
    - name: jaeger-compact
      port: 6831
      targetPort: 6831
      protocol: UDP
    - name: jaeger-grpc
      port: 14250
      targetPort: 14250
      protocol: TCP
    - name: jaeger-thrift
      port: 14268
      targetPort: 14268
      protocol: TCP
    - name: otlp
      port: 4317
      targetPort: 4317
      protocol: TCP
      appProtocol: grpc
    - name: otlp-http
      port: 4318
      targetPort: 4318
      protocol: TCP
    - name: zipkin
      port: 9411
      targetPort: 9411
      protocol: TCP
  selector:
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: release-name
    component: standalone-collector
  internalTrafficPolicy: Cluster
---
# Source: kubiya-runner/templates/agent-manager-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: agent-manager
  namespace: kubiya
  labels:    
    helm.sh/chart: kubiya-runner-0.1.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: agent-manager
spec:
  selector:    
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: agent-manager
  ports:
    - name: http
      port: 80
      targetPort: 8080
  type: "ClusterIP"
---
# Source: kubiya-runner/templates/tool-manager-service.yaml
apiVersion: v1
kind: Service
metadata:
  name: tool-manager
  namespace: kubiya
  labels:    
    helm.sh/chart: kubiya-runner-0.1.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: tool-manager
spec:
  selector:    
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: tool-manager
  ports:
    - name: http
      port: 80
      targetPort: 3001
  type: "ClusterIP"
---
# Source: kubiya-runner/charts/dagger/templates/engine-daemonset.yaml
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: release-name-dagger-engine
  namespace: kubiya
  labels:
    helm.sh/chart: dagger-0.13.6
    app.kubernetes.io/name: dagger
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: v0.13.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/part-of: dagger
spec:
  selector:
    matchLabels:
      name: release-name-dagger-engine
  template:
    metadata:
      annotations:
        checksum/config: 6b395af93488280f99982e6360fe2c5f95a0e1170a3fe632081c9269ead0a3e4
      labels:
        name: release-name-dagger-engine
        helm.sh/chart: dagger-0.13.6
        app.kubernetes.io/name: dagger
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: v0.13.6
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/part-of: dagger
    spec:
      securityContext:
        runAsUser: 0
        runAsGroup: 1001
        fsGroup: 1001
        fsGroupChangePolicy: "OnRootMismatch"
      serviceAccountName: default
      containers:
        - name: dagger-engine
          image: registry.dagger.io/engine:v0.13.6
          imagePullPolicy: IfNotPresent
          securityContext:
            privileged: true
            capabilities:
              add:
                - ALL
          resources:
            limits:
              cpu: "4"
              memory: 4Gi
            requests:
              cpu: "1"
              memory: 1Gi
          readinessProbe:
            exec:
              command:
                - sh
                - -exc
                - |-
                  dagger core version
            failureThreshold: 10
            initialDelaySeconds: 5
            periodSeconds: 15
            successThreshold: 1
            timeoutSeconds: 14
          volumeMounts:
            - name: varlibdagger
              mountPath: /var/lib/dagger
            - name: varrundagger
              mountPath: /var/run/buildkit
            - name: dagger-engine-config
              mountPath: /etc/dagger/engine.toml
              subPath: engine.toml
      terminationGracePeriodSeconds: 300
      volumes:
        - name: varlibdagger
          hostPath:
            path: /var/lib/dagger
        - name: varrundagger
          hostPath:
            path: /var/run/dagger
        - name: dagger-engine-config
          configMap:
            name: release-name-dagger-engine-config
            items:
              - key: engine.toml
                path: engine.toml
---
# Source: kubiya-runner/charts/kubeStateMetrics/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-kubeStateMetrics
  namespace: kubiya
  labels:    
    helm.sh/chart: kubeStateMetrics-5.27.0
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/component: metrics
    app.kubernetes.io/part-of: kubeStateMetrics
    app.kubernetes.io/name: kubeStateMetrics
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "2.14.0"
spec:
  selector:
    matchLabels:      
      app.kubernetes.io/name: kubeStateMetrics
      app.kubernetes.io/instance: release-name
  replicas: 1
  strategy:
    type: Recreate
  revisionHistoryLimit: 2
  template:
    metadata:
      labels:        
        helm.sh/chart: kubeStateMetrics-5.27.0
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/component: metrics
        app.kubernetes.io/part-of: kubeStateMetrics
        app.kubernetes.io/name: kubeStateMetrics
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/version: "2.14.0"
    spec:
      automountServiceAccountToken: true
      hostNetwork: false
      serviceAccountName: release-name-kubeStateMetrics
      securityContext:
        fsGroup: 65534
        runAsGroup: 65534
        runAsNonRoot: true
        runAsUser: 65534
        seccompProfile:
          type: RuntimeDefault
      containers:
      - name: kubeStateMetrics
        args:
        - --port=8080
        - --resources=certificatesigningrequests,configmaps,cronjobs,daemonsets,deployments,endpoints,horizontalpodautoscalers,ingresses,jobs,leases,limitranges,mutatingwebhookconfigurations,namespaces,networkpolicies,nodes,persistentvolumeclaims,persistentvolumes,poddisruptionbudgets,pods,replicasets,replicationcontrollers,resourcequotas,secrets,services,statefulsets,storageclasses,validatingwebhookconfigurations,volumeattachments
        - --namespaces=kubiya
        - --telemetry-host=0.0.0.0
        - --telemetry-port=8081
        imagePullPolicy: IfNotPresent
        image: registry.k8s.io/kube-state-metrics/kube-state-metrics:2.14.0
        ports:
        - containerPort: 8080
          name: "http"
        - containerPort: 8081
          name: "metrics"
        livenessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /livez
            port: 8080
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        readinessProbe:
          failureThreshold: 3
          httpGet:
            httpHeaders:
            path: /readyz
            port: 8081
            scheme: HTTP
          initialDelaySeconds: 5
          periodSeconds: 10
          successThreshold: 1
          timeoutSeconds: 5
        resources:
          limits:
            cpu: 400m
            memory: 256Mi
          requests:
            cpu: 10m
            memory: 32Mi
        securityContext:
          allowPrivilegeEscalation: false
          capabilities:
            drop:
            - ALL
          readOnlyRootFilesystem: true
---
# Source: kubiya-runner/charts/opentelemetry-collector/templates/deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: release-name-opentelemetry-collector
  namespace: kubiya
  labels:
    helm.sh/chart: opentelemetry-collector-0.109.0
    app.kubernetes.io/name: opentelemetry-collector
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/version: "0.113.0"
    app.kubernetes.io/managed-by: Helm
    
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      app.kubernetes.io/name: opentelemetry-collector
      app.kubernetes.io/instance: release-name
      component: standalone-collector
  strategy:
    type: RollingUpdate
  template:
    metadata:
      annotations:
        checksum/config: 01ba4719c80b6fe911b091a7c05124b64eeece964e09c058ef8f9805daca546b
        
      labels:
        app.kubernetes.io/name: opentelemetry-collector
        app.kubernetes.io/instance: release-name
        component: standalone-collector
        
    spec:
      
      serviceAccountName: otel-collector-sa-token
      securityContext:
        {}
      containers:
        - name: opentelemetry-collector
          args:
            - --config=/conf/relay.yaml
          securityContext:
            {}
          image: "ghcr.io/kubiyabot/otel-connector:release-0.1.1"
          imagePullPolicy: IfNotPresent
          ports:
            
            - name: jaeger-compact
              containerPort: 6831
              protocol: UDP
            - name: jaeger-grpc
              containerPort: 14250
              protocol: TCP
            - name: jaeger-thrift
              containerPort: 14268
              protocol: TCP
            - name: otlp
              containerPort: 4317
              protocol: TCP
            - name: otlp-http
              containerPort: 4318
              protocol: TCP
            - name: zipkin
              containerPort: 9411
              protocol: TCP
          env:
            - name: MY_POD_IP
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: GOMEMLIMIT
              value: "3276MiB"
          livenessProbe:
            httpGet:
              path: /
              port: 13133
          readinessProbe:
            httpGet:
              path: /
              port: 13133
          resources:
            limits:
              cpu: "2"
              memory: 4Gi
            requests:
              cpu: 250m
              memory: 256Mi
          volumeMounts:
            - mountPath: /conf
              name: opentelemetry-collector-configmap
            - mountPath: /opt
              name: nats-creds-volume
              readOnly: true
      volumes:
        - name: opentelemetry-collector-configmap
          configMap:
            name: otel-collector-config
            items:
              - key: relay
                path: relay.yaml
        - name: nats-creds-volume
          secret:
            items:
            - key: nats.creds
              path: nats.creds
            secretName: nats-creds
      hostNetwork: false
---
# Source: kubiya-runner/templates/agent-manager-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: agent-manager
  namespace: kubiya
  labels:    
    helm.sh/chart: kubiya-runner-0.1.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: agent-manager
spec:
  replicas: 2
  selector:
    matchLabels:      
      app.kubernetes.io/name: kubiya-runner
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: agent-manager
  template:
    metadata:
      labels:        
        helm.sh/chart: kubiya-runner-0.1.6
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kubiya-runner
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/component: agent-manager
    spec:
      containers:
        - name: agent-manager
          image: "ghcr.io/kubiyabot/agent-manager:0.0.15"
          imagePullPolicy: "Always"
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 200m
              memory: 256Mi
          env:
            - name: "TOOL_MANAGER_URL"
              value: "tool-manager:3001"
            - name: KUBIYA_NATS_CREDS
              value: /nats/nats.creds
            - name: ENVIRONMENT
              value: production
          ports:
            - containerPort: 8080
          volumeMounts:
            - mountPath: /nats/
              name: nats-creds-volume
              readOnly: true
      volumes:
        - name: nats-creds-volume
          secret:
            defaultMode: 420
            items:
            - key: nats.creds
              path: nats.creds
            secretName: nats-creds-customer
---
# Source: kubiya-runner/templates/kubiya-operator-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: kubiya-operator
  namespace: kubiya
  labels:    
    helm.sh/chart: kubiya-runner-0.1.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: kubiya-operator
spec:
  replicas: 1
  selector:
    matchLabels:      
      app.kubernetes.io/name: kubiya-runner
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: kubiya-operator
  template:
    metadata:
      labels:        
        helm.sh/chart: kubiya-runner-0.1.6
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kubiya-runner
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/component: kubiya-operator
    spec:
      serviceAccountName:         kubiya-runner
      containers:
        - name: kubiya-operator
          image: "ghcr.io/kubiyabot/kubiya-operator:latest"
          imagePullPolicy: "Always"
          ports:
            - containerPort: 80
          env:
              - name: "NATS_SERVER"
                value: ""
              - name: "NATS_SUBJECT"
                value: ""
              - name: "NATS_CREDENTIAL_FILE"
                value: "/etc/nats/nats.creds"
              - name: "NAMESPACE"
                value: "kubiya"
          volumeMounts:
            - mountPath: /etc/nats
              name: runner-secret-volume
              readOnly: true
      volumes:
        - name: runner-secret-volume
          secret:
            items:
            - key: nats.creds
              path: nats.creds
            secretName: runner-secret
---
# Source: kubiya-runner/templates/tool-manager-deployment.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: tool-manager
  namespace: kubiya
  labels:
    
    helm.sh/chart: kubiya-runner-0.1.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: tool-manager
spec:
  replicas: 3
  selector:
    matchLabels:
      
      app.kubernetes.io/name: kubiya-runner
      app.kubernetes.io/instance: release-name
      app.kubernetes.io/component: tool-manager
  template:
    metadata:
      labels:
        
        helm.sh/chart: kubiya-runner-0.1.6
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kubiya-runner
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/component: tool-manager
    spec:
      serviceAccountName: kubiya-runner
      containers:
        - name: tool-manager
          image: "ghcr.io/kubiyabot/tool-manager:2a689e758f0d4c9aeeb464c4847b8749409cf8d2"
          imagePullPolicy: "Always"
          resources:
            limits:
              cpu: "1"
              memory: 1Gi
            requests:
              cpu: 200m
              memory: 256Mi
          command: 
          - /bin/bash
          - -c
          - mkdir -p /usr/local/share/ca-certificates && cp /etc/docker/certs.d/cache-registry-svc.kubiya/tls.crt
            /usr/local/share/ca-certificates/registry.crt && update-ca-certificates && /bin/bash
            /start_tools_server.sh
          env:
            - name: GIT_SHA
              value: "git-sha-placeholder"
            - name: KUBIYA_AGENT_kubiyaAgentUUID
              value: "{{ .Values.kubiyaAgentUUID }}"
            - name: KUBIYA_IMAGE_REGISTRY_ADDRESS
              value: "cache-registry-svc.kubiya"
            - name: KUBIYA_NATS_CREDS
              value: "/opt/nats.creds"
            - name: KUBIYA_SDK_SERVER_URL
              value: "http://127.0.0.1:8000"
            - name: KUBIYA_SERVICE_ACCOUNT
              value: "kubiya-service-account"
            - name: KUBIYA_TOOLS_SHARED_VOLUME
              value: "/tmp/kubiya_shared_tools"
            - name: KUBIYA_TOOL_TIMEOUT
              value: "168h"
            - name: KUBIYA_USER_ORG
              value: "{{ .Values.organization }}"
          ports:
            - containerPort: 
          volumeMounts:
            - mountPath: /opt/
              name: nats-creds-volume
            - mountPath: /tmp/kubiya_shared_tools
              name: shared-volume
            - mountPath: /etc/docker/certs.d/cache-registry-svc.kubiya
              name: registry-certs

        - name: sdk-server
          image: "ghcr.io/kubiyabot/sdk-py:v0.45.1"
          imagePullPolicy: "Always"
          resources:
            limits:
              cpu: 500m
              memory: 512Mi
            requests:
              cpu: 100m
              memory: 128Mi
          command: 
          - python
          - -m
          - kubiya_sdk
          - server
          - --host
          - 0.0.0.0
          - --port
          - "8000"
          env:
            - name: "GIT_SHA"
              value: "git-sha-placeholder"
            - name: "KUBIYA_AGENT_kubiyaAgentUUID"
              value: "{{ .Values.kubiyaAgentUUID }}"
            - name: "KUBIYA_NATS_CREDS"
              value: "/opt/nats.creds"
            - name: "KUBIYA_SERVICE_ACCOUNT"
              value: "kubiya-service-account"
            - name: "KUBIYA_TOOLS_SHARED_VOLUME"
              value: "/tmp/kubiya_shared_tools"
            - name: "KUBIYA_TOOL_TIMEOUT"
              value: "168h"
            - name: "KUBIYA_USER_ORG"
              value: "{{ .Values.organization }}"
          ports:
            - containerPort: 8000
          volumeMounts:
            - mountPath: /opt/
              name: nats-creds-volume
            - mountPath: /tmp/kubiya_shared_tools
              name: shared-volume
            - mountPath: /etc/docker/certs.d/cache-registry-svc.kubiya
              name: registry-certs
      volumes:
        - name: nats-creds-volume
          secret:
            items:
            - key: nats.creds
              path: nats.creds
            secretName: nats-creds
        - emptyDir: {}
          name: shared-volume
        - name: null
          secret:
            defaultMode: 420
            secretName: registry-tls-secret
---
# Source: kubiya-runner/templates/image-updater-cronjob.yaml
apiVersion: batch/v1
kind: CronJob
metadata:
  name: release-name-kubiya-runner-image-updater
  namespace: kubiya
  labels:    
    helm.sh/chart: kubiya-runner-0.1.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
    app.kubernetes.io/component: image-updater
spec:
  schedule: "0 * * * *"
  jobTemplate:
    metadata:
      labels:        
        helm.sh/chart: kubiya-runner-0.1.6
        app.kubernetes.io/managed-by: Helm
        app.kubernetes.io/name: kubiya-runner
        app.kubernetes.io/instance: release-name
        app.kubernetes.io/component: image-updater
    spec:
      template:
        metadata:
          labels:            
            app.kubernetes.io/name: kubiya-runner
            app.kubernetes.io/instance: release-name
            app.kubernetes.io/component: image-updater
        spec:
          securityContext:
            runAsUser: 0
            runAsGroup: 0
          containers:
          - name: image-updater
            image: bitnami/kubectl:latest
            command: ["/bin/bash"]
            args:
            - -c
            - |
              set -e
              echo "Starting image updater job"
              echo "Installing curl and jq..."
              install_packages curl jq
              echo "Downloading JSON file from S3..."
              MAX_RETRIES=3
              RETRY_DELAY=5
              for i in $(seq 1 $MAX_RETRIES); do
                if curl -s -f https://kubiya-cli.s3.amazonaws.com/stable/kubiya_versions.json > /tmp/kubiya_versions.json; then
                  break
                fi
                echo "Attempt $i failed. Retrying in $RETRY_DELAY seconds..."
                sleep $RETRY_DELAY
              done
              if [ $i -eq $MAX_RETRIES ]; then
                echo "Error: Failed to download JSON file after $MAX_RETRIES attempts"
                exit 1
              fi
              if ! jq empty /tmp/kubiya_versions.json; then
                echo "Error: Invalid JSON file downloaded from S3"
                echo "File contents:"
                cat /tmp/kubiya_versions.json
                exit 1
              fi
              update_deployment() {
                local deployment=$1
                local container=$2
                local new_image=$3
                echo "Checking deployment $deployment, container $container"
                current_image=$(kubectl get deployment $deployment -n kubiya -o jsonpath="{.spec.template.spec.containers[?(@.name=='$container')].image}")
                if [ "$current_image" != "$new_image" ]; then
                  echo "Updating $deployment deployment, container $container with new image: $new_image"
                  kubectl set image deployment/$deployment -n kubiya $container=$new_image
                else
                  echo "Deployment $deployment, container $container is already up to date"
                fi
              }
              echo "Updating deployments..."
              tool_manager_image=$(jq -r '."tool-manager"' /tmp/kubiya_versions.json)
              update_deployment "tool-manager" "tool-manager" "$tool_manager_image"
              agent_manager_image=$(jq -r '."agent-manager"' /tmp/kubiya_versions.json)
              update_deployment "agent-manager" "agent-manager" "$agent_manager_image"
              sdk_py_image=$(jq -r '."sdk-py"' /tmp/kubiya_versions.json)
              update_deployment "tool-manager" "kubiya-sdk-server" "$sdk_py_image"
              echo "Image updater job completed successfully"
          serviceAccountName: image-updater-sa
          restartPolicy: OnFailure
---
# Source: kubiya-runner/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: kubiya-runner-nats-jwt-secret
  namespace: kubiya
  labels:
    helm.sh/chart: kubiya-runner-0.1.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
  annotations:
    "helm.sh/hook": "pre-install,pre-upgrade"
type: Opaque
data:
  nats.creds: "LS0tLS1CRUdJTiBOQVRTIFVTRVIgSldULS0tLS0KZXlKMGVYQWlPaUpLVjFRaUxDSmhiR2NpT2lKbFpESTFOVEU1TFc1clpYa2lmUS5leUpxZEdraU9pSk5TVXhXUkVvME0wWktURUkzTWtOWVExcEpVRnBHTWtsRk0wdENURkZFVDBwSE56VldSbFV5VGswMFRWZFZNbE5YVmpKUklpd2lhV0YwSWpveE56TXdOak0xTXpVMkxDSnBjM01pT2lKQlExUkJVVm8yVFZkU1YwdE9SRWxLUlVaTFRFSkpSakpNUkZCVlJrdFdTMU5PTlVkTVUwdEhVa1l6TjFKSU1rVlVWVmxOVTAxQ1ZDSXNJbTVoYldVaU9pSnJkV0pwZVdFdGRHVnpkQzF6Ym1sbVppNXpaWEpuWlhrdGJXVjBjbWxqY3kxMFpYTjBJaXdpYzNWaUlqb2lWVUpKVEVWRldrUk5SVE5RVVU0elRWRlBURnBCV0VaUE0xSlNUekl5VUVGT1QxVkdWRlpNVlVOVFFWZEVSa05UUXpaWk5GWldOek1pTENKdVlYUnpJanA3SW5CMVlpSTZlMzBzSW5OMVlpSTZlMzBzSW1semMzVmxjbDloWTJOdmRXNTBJam9pUVVGWVQxQlVOVTFIU2t0VVN6WTJVRGROVnpKU1VrbE1VVmhJUXpaRVdrdFVWRTgyU0ZkRVdFOUxTbEZPUlVsSlNVOU5WVm8wUWxNaUxDSjBlWEJsSWpvaWRYTmxjaUlzSW5abGNuTnBiMjRpT2pKOWZRLmlsT21KcWx5c1ZwV2l1SDAtZVdwOXYzZFF5WE5zY0NaT1VwNW55LVYwd3hTWWVoTWlqcHdJX18xMUs0MUdHLVZYTXpCNUNtR3FOZlY1M1dMV1FtOEFnCi0tLS0tLUVORCBOQVRTIFVTRVIgSldULS0tLS0tCgoqKioqKioqKioqKioqKioqKioqKioqKioqIElNUE9SVEFOVCAqKioqKioqKioqKioqKioqKioqKioqKioqCk5LRVkgU2VlZCBwcmludGVkIGJlbG93IGNhbiBiZSB1c2VkIHRvIHNpZ24gYW5kIHByb3ZlIGlkZW50aXR5LgpOS0VZcyBhcmUgc2Vuc2l0aXZlIGFuZCBzaG91bGQgYmUgdHJlYXRlZCBhcyBzZWNyZXRzLgoKLS0tLS1CRUdJTiBVU0VSIE5LRVkgU0VFRC0tLS0tClNVQUpYWjNQQklSRVpMNE1VWjRVUFBBV1E0M0FPU1pGSUdITTJQSEczNU5XSUVHNjdWNkNMV0hMTkEKLS0tLS0tRU5EIFVTRVIgTktFWSBTRUVELS0tLS0tCgoqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqCg=="
---
# Source: kubiya-runner/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: kubiya-runner-nats-second-jwt-secret
  namespace: kubiya
  labels:
    helm.sh/chart: kubiya-runner-0.1.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
  annotations:
    "helm.sh/hook": "pre-install,pre-upgrade"
type: Opaque
data:
  nats.creds: "LS0tLS1CRUdJTiBOQVRTIFVTRVIgSldULS0tLS0KZXlKMGVYQWlPaUpLVjFRaUxDSmhiR2NpT2lKbFpESTFOVEU1TFc1clpYa2lmUS5leUpxZEdraU9pSXpUMEZCUWtKWlNWTk1RemREVkRORlNGZFVWVlZhVWpkQlQxWkVUalZDUmt0T1UwWlFWMWRKVVVaRFVGbEhTbFF6VmxGUklpd2lhV0YwSWpveE56SXdNREF5TkRNekxDSnBjM01pT2lKQlJFMDJOMUphVmxwUk5FaFBVMGhPTWxORVNsQlVRbFpZTTAwM1dWaEpUVE15TkRkS1JVeFZVRFJRVHpWT1dVbEpORmRLUjB0VVZpSXNJbTVoYldVaU9pSnJkV0pwZVdFdGRHVnpkQzF6Ym1sbVppSXNJbk4xWWlJNklsVkVRa00xVkVwS1MwNVBTbFl6UVZWTldUZEpUbFJCUWpaYVYxVk1UbEJNTkV0VFdWWk9VemRMU2xjelJUZExSME5FU1ZwUVFVOUpJaXdpYm1GMGN5STZleUp3ZFdJaU9udDlMQ0p6ZFdJaU9udDlMQ0pwYzNOMVpYSmZZV05qYjNWdWRDSTZJa0ZCV0U5UVZEVk5SMHBMVkVzMk5sQTNUVmN5VWxKSlRGRllTRU0yUkZwTFZGUlBOa2hYUkZoUFMwcFJUa1ZKU1VsUFRWVmFORUpUSWl3aWRIbHdaU0k2SW5WelpYSWlMQ0oyWlhKemFXOXVJam95ZlgwLmdBbmo1Tnc4aXpvWjNTZXB2U1VzandEZ2xaSlVEQ0p2WGlPb3pXemRocGhGdG03TElud1JoclhTS05RcE9UeUpadm5XM2QyYWg0bmZIMzFBVkF1S0J3Ci0tLS0tLUVORCBOQVRTIFVTRVIgSldULS0tLS0tCgoqKioqKioqKioqKioqKioqKioqKioqKioqIElNUE9SVEFOVCAqKioqKioqKioqKioqKioqKioqKioqKioqCk5LRVkgU2VlZCBwcmludGVkIGJlbG93IGNhbiBiZSB1c2VkIHRvIHNpZ24gYW5kIHByb3ZlIGlkZW50aXR5LgpOS0VZcyBhcmUgc2Vuc2l0aXZlIGFuZCBzaG91bGQgYmUgdHJlYXRlZCBhcyBzZWNyZXRzLgoKLS0tLS1CRUdJTiBVU0VSIE5LRVkgU0VFRC0tLS0tClNVQUdCMzNYMkdFVEJOQldZU1ZNTDRDQVFQNUpJWFpKTzJKV0ZaUEpWRkczQTdJUDdFVFRTWk9VNDQKLS0tLS0tRU5EIFVTRVIgTktFWSBTRUVELS0tLS0tCgoqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqKioqCg=="
---
# Source: kubiya-runner/templates/secrets.yaml
apiVersion: v1
kind: Secret
metadata:
  name: kubiya-runner-registry-tls-secret
  namespace: kubiya
  labels:
    helm.sh/chart: kubiya-runner-0.1.6
    app.kubernetes.io/managed-by: Helm
    app.kubernetes.io/name: kubiya-runner
    app.kubernetes.io/instance: release-name
  annotations:
    "helm.sh/hook": "pre-install,pre-upgrade"
type: kubernetes.io/tls
data:
  tls.crt: "LS0tLS1CRUdJTiBDRVJUSUZJQ0FURS0tLS0tCk1JSUQ4VENDQXRtZ0F3SUJBZ0lVTVBIY1RMN1FGNmMrYlhtNElxTEUzeXVhK1lrd0RRWUpLb1pJaHZjTkFRRUwKQlFBd2NURUxNQWtHQTFVRUJoTUNWVk14Q3pBSkJnTlZCQWdNQWtOQk1SSXdFQVlEVlFRSERBbFRkVzV1ZVhaaApiR1V4RHpBTkJnTlZCQW9NQmt0MVltbDVZVEVNTUFvR0ExVUVDd3dEVWs1RU1TSXdJQVlEVlFRRERCbGpZV05vClpTMXlaV2RwYzNSeWVTMXpkbU11YTNWaWFYbGhNQjRYRFRJME1UQXdNakV5TkRZeU4xb1hEVEkzTURrd01qRXkKTkRZeU4xb3djVEVMTUFrR0ExVUVCaE1DVlZNeEN6QUpCZ05WQkFnTUFrTkJNUkl3RUFZRFZRUUhEQWxUZFc1dQplWFpoYkdVeER6QU5CZ05WQkFvTUJrdDFZbWw1WVRFTU1Bb0dBMVVFQ3d3RFVrNUVNU0l3SUFZRFZRUUREQmxqCllXTm9aUzF5WldkcGMzUnllUzF6ZG1NdWEzVmlhWGxoTUlJQklqQU5CZ2txaGtpRzl3MEJBUUVGQUFPQ0FROEEKTUlJQkNnS0NBUUVBM3c5b2dVUGptaVNaMDR1OU4vR3hKRmhCT01QYzQ3MXJLR1ozZVc2bW43NW1FTjNzV2R1cwpzVStkK1lNSE9HYjBUT3FlcHhMUUtvMWNRYjVRUmlRTGxOVTJWRGgrTjVhZVVmdVJUU1MwczNRWTAxd1k1Z3ozCnIvaFc3ZllEOVVpTUpTeFhYK01DU2hsQ0NTUm52N2hncmloQWlGdXcyZUs3bjQyWjByYms2MUJmaGNBRWxZbmEKdExoRjBwcWxvNGlsSHZaUGEwdzRFMzJBdkJJTzZ5YjlKb1V0N1JRMVNta3VLRDJsY2dqMVlKSjQ2SmxKUGtFWgowL21Zc3ZwZUZBOVBmQnZGdkZ6K0VyMitMd09JKzcxNFRtTnNITXVERjRyNzVNeFF5ekpaYTNxT3Rud3paTEVYCjlHYThHZ1AxVG9jVGtlRnVMUmhjVWQ4M3M5eWJIS0o1bHdJREFRQUJvNEdBTUg0d0RnWURWUjBQQVFIL0JBUUQKQWdXZ01CTUdBMVVkSlFRTU1Bb0dDQ3NHQVFVRkJ3TUJNRGdHQTFVZEVRUXhNQytDR1dOaFkyaGxMWEpsWjJsegpkSEo1TFhOMll5NXJkV0pwZVdHQ0VtTmhZMmhsTFhKbFoybHpkSEo1TFhOMll6QWRCZ05WSFE0RUZnUVVxQlpGCkZKZEZLQ1M0SGtSYWxtNmM1WFFORlJVd0RRWUpLb1pJaHZjTkFRRUxCUUFEZ2dFQkFHYXlraDhhV3QwaTdpNnMKOGZZRHlZVE8wMjZOYjQ5dWNUZDRpKzgySGFCdm01ZFJxbVQ5U0V4eFJYL3RTanMraGpHRnpYTWVtY0lva3FKLwpFNWtSYlcrbFNveWUwQkdkTVBPcVJNZW1tcEI4WG9DZmo0ZWg0WkxtbDRrbGVSZEEzRU96OHJSRExxb0MxdmJiClluU2RwTjFHbUFJOUtpdjJ0N3hjVVl0Nm1SM2FqYVBLZHVWOXNkakRrZDBDQm1EckhNRXJ4a3AzdDVXOHc5aW8KN2IxRjE3MFBSRldJNVY3c3JtQlFwK0t6UTJsZEdyODdxZFI1eE1wZlU4Z0IvKzgyRXlZcjA1WTJsNmhrY1BGbgpzMnBPTUV4UFVVN05xQlMxeURoR3lXZVVJTE9ydmFuWjh3L1UyVDFScjd1bTBGK0Y0a3hTMHZNMDhyYmRIMzNPCjVyUDV1OEk9Ci0tLS0tRU5EIENFUlRJRklDQVRFLS0tLS0K"
  tls.key: "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"
