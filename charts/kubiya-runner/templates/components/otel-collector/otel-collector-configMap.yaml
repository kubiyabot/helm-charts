apiVersion: v1
kind: ConfigMap
metadata:
  name: kubiya-runner-otel-collector-config
  namespace: {{ .Release.Namespace }}
  labels:
    {{ include "otel-collector-extras.labels" . | nindent 4 }}
data:
  config.yaml: |
    receivers:
      prometheus:
        config:
          scrape_configs:
            # Kube-state-metrics with service discovery
            - job_name: 'kube-state-metrics'
              scrape_interval: 30s
              kubernetes_sd_configs:
                - role: endpoints
                  namespaces:
                    names: ["{{ .Release.Namespace }}"]
              relabel_configs:
                # Keep only kube-state-metrics endpoints
                - source_labels: [__meta_kubernetes_service_label_app_kubernetes_io_name]
                  regex: .*kubestatemetrics.*
                  action: keep

            # Add prometheus receiver for self-monitoring
            - job_name: 'otel-collector'
              scrape_interval: 60s
              static_configs:
                - targets: ['${env:MY_POD_IP}:8888']

            # Dynamic endpoint discovery for Kubiya apps
            - job_name: "kubiya-apps"
              scrape_interval: 30s
              kubernetes_sd_configs:
                - role: endpoints
                  namespaces:
                    names: ["{{ .Release.Namespace }}"]
              relabel_configs:
                # Keep only endpoints with kubiya.io/metrics-name label
                - source_labels: [__meta_kubernetes_service_label_kubiya_io_metrics_name]
                  action: keep
                  regex: (.+)
                # Add app name from the label
                - source_labels: [__meta_kubernetes_service_label_kubiya_io_metrics_name]
                  target_label: app

            # Add blackbox-exporter probes metrics scraping
            - job_name: 'blackbox-http'
              metrics_path: /probe
              params:
                module: [http_2xx]
              static_configs:
                - targets: {{ .Values.blackboxexporter.targets.http | toJson }}
              relabel_configs:
                - source_labels: [__address__]
                  target_label: __param_target
                - source_labels: [__param_target]
                  target_label: instance
                - target_label: __address__
                  replacement: prometheus-blackbox-exporter:9115

            - job_name: 'blackbox-grpc'
              metrics_path: /probe
              params:
                module: [grpc]
              static_configs:
                - targets: {{ .Values.blackboxexporter.targets.grpc | toJson }}
              relabel_configs:
                - source_labels: [__address__]
                  target_label: __param_target
                - source_labels: [__param_target]
                  target_label: instance
                - target_label: __address__
                  replacement: prometheus-blackbox-exporter:9115

            - job_name: 'blackbox-tcp'
              metrics_path: /probe
              params:
                module: [tcp_connect]
              static_configs:
                - targets: {{ .Values.blackboxexporter.targets.tcp | toJson }}
              relabel_configs:
                - source_labels: [__address__]
                  target_label: __param_target
                - source_labels: [__param_target]
                  target_label: instance
                - target_label: __address__
                  replacement: prometheus-blackbox-exporter:9115

    # Extension for exposing health checks endpoint. Provides liveness and readiness probes (required for Gatekeeper as well).
    extensions:
      health_check:
        endpoint: ${env:MY_POD_IP}:13133

    processors:
      {{- if .Values.otelcollector.k8sAttributes.enabled | default true }}
      # Processor for Kubernetes attributes, enriching metrics with Kubernetes metadata.
      k8sattributes:
        auth_type: serviceAccount
        filter:
          namespace: {{ .Release.Namespace }}
          # TODO: Require latest public otel-collector version, or our custom image rebuild from updated sources
          # wait_for_metadata: true
          # wait_for_metadata_timeout: 10s
          # passthrough: false
          # ---------------------------------------------------------------------------------------------------
        extract:
          # The attributes provided in 'metadata' will be added to associated resources
          metadata:
          - k8s.namespace.name
          - k8s.deployment.name
          - k8s.statefulset.name
          - k8s.daemonset.name
          - k8s.cronjob.name
          - k8s.job.name
          - k8s.node.name
          - k8s.pod.name
          - k8s.pod.uid
          - k8s.pod.start_time
          labels:
          # This label extraction rule takes the value 'app.kubernetes.io/component' label and maps it to the 'app.label.component' attribute which will be added to the associated resources
          - tag_name: app.label.component
            key: app.kubernetes.io/component
            from: pod
        pod_association:
        - sources:
          # This rule associates all resources containing the 'k8s.pod.ip' attribute with the matching pods. If this attribute is not present in the resource, this rule will not be able to find the matching pod.
          - from: resource_attribute
            name: k8s.pod.ip
        - sources:
          # This rule associates all resources containing the 'k8s.pod.uid' attribute with the matching pods. If this attribute is not present in the resource, this rule will not be able to find the matching pod.
          - from: resource_attribute
            name: k8s.pod.uid
        - sources:
          # This rule will use the IP from the incoming connection from which the resource is received, and find the matching pod, based on the 'pod.status.podIP' of the observed pods
          - from: connection
      {{- end }}

      # Processor for batching metrics. Optimized for NATS/Synadia.
      batch/metrics:
        # Aims for ~100KB batches; default is 8192 messages
        send_batch_size: 512
        # Prevents exceeding NATS limits; default is 0 (unlimited messages)
        send_batch_max_size: 1024
        # For latency/throughput expected to be better with 128KB batches; default is 8192 bytes
        # TODO: Require latest public otel-collector version, or our custom image rebuild from updated sources
        # send_batch_max_bytes: 128000
        # Batch sending interval. Default is "200ms"
        timeout: "10s"
        # TODO: Require latest public otel-collector version, or our custom image rebuild from updated sources
        # Disable batch timeout. Default is "0s" (disabled)
        # send_batch_timeout: "5s"

      # TODO PROD_READY: Remove this after testing
      # resource/self-metrics:
      #   attributes:
      #   - action: insert
      #     key: k8s.pod.ip
      #     value: ${env:MY_POD_IP}

      # transform/collision:
      #   metric_statements:
      #   - context: datapoint
      #     statements:
      #     - set(attributes["exported_location"], attributes["location"])
      #     - delete_key(attributes, "location")
      #     - set(attributes["exported_cluster"], attributes["cluster"])
      #     - delete_key(attributes, "cluster")
      #     - set(attributes["exported_namespace"], attributes["namespace"])
      #     - delete_key(attributes, "namespace")
      #     - set(attributes["exported_job"], attributes["job"])
      #     - delete_key(attributes, "job")
      #     - set(attributes["exported_instance"], attributes["instance"])
      #     - delete_key(attributes, "instance")
      #     - set(attributes["exported_project_id"], attributes["project_id"])
      #     - delete_key(attributes, "project_id")
      # -------------------------------
          
      # Adding additional runner's metadata added to 'resourceMetrics.resource.attributes' level of json sent to NATS)
      # This will be appended to all data types (logs, metrics, traces).
      # IMPORTANT: This legacy processor is disabled currently as it found to set duplicates of attributes of metricstransform processor cfg, 
      # while appending them on higher level of json structure, which is not available in metrics delivered to the storage backend (Prometheus).
      {{- if .Values.otelcollector.resourceAttributes.enabled | default true }}
      resource:
        attributes:
          - key: "instance"
            value: {{ .Values.organization | quote }}
            action: upsert

          - key: "runner"
            value: {{ .Values.runnerNameOverride | default .Release.Name | quote }}
            action: upsert

          - key: "organization"
            value: {{ .Values.organization | quote }}
            action: upsert

          - key: "kubiya_type"
            value: "customers-runners"
            action: upsert
      {{- end }}

      # TODO: Waiting for Mevrat response to making sure this is not duplicated resource.attributes
      {{- if .Values.otelcollector.metricstransformAttributes.enabled | default true }}
      metricstransform:
        transforms:
          - include: '.*'
            match_type: regexp
            action: update
            operations:
              - action: update_label
                label: "instance"
                new_label: "instance"
                new_value: {{ .Values.organization | quote }}

              - action: add_label
                label: "kubiya_type"
                new_label: "kubiya_type"
                new_value: "customers-runners"

              - action: add_label
                label: "runner"
                new_label: "runner"
                new_value: {{ .Values.runnerNameOverride | default .Release.Name | quote }}

              - action: add_label
                label: "organization"
                new_label: "organization"
                new_value: {{ .Values.organization | quote }}
      {{- end }}

    # Exporter for debugging output
    exporters:
      {{- if .Values.otelcollector.debugExporter.enabled }}
      debug:
        verbosity: normal
        # sampling_initial: 5
        # sampling_thereafter: 200
      {{- end }}

      {{- if .Values.otelcollector.prometheusExporter.enabled }}
      prometheusremotewrite:
        endpoint: {{ .Values.otelcollector.prometheusExporter.endpoint | default "0.0.0.0:8889" | quote }}
        resource_to_telemetry_conversion:
          enabled: true
        enable_open_metrics: true
        remote_write:
          - url: {{ required "Azure Managed Prometheus endpoint URL is required" .Values.otelcollector.prometheusExporter.remoteWrite.url | quote }}
            headers:
              Authorization: "Bearer {{ .Values.otelcollector.prometheusExporter.managedPrometheusAuth.token | quote }}"
      {{- end }}

      natsexporter:
        org: {{ .Values.organization | quote }}
        endpoint: {{ .Values.nats.serverUrl | quote }}
        creds_file: "/etc/nats/nats.creds"

    service:
      extensions:
        - health_check

      pipelines:
        {{ .Values.otelcollector.pipelines | toYaml | nindent 10 }}
      telemetry:
        metrics:
          level: basic
          address: ${env:MY_POD_IP}:8888
